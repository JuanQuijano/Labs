# Use Delta Lake in Azure Databricks #
Delta Lake is an open source project to build a transactional data storage layer for Spark on top of a data lake. Delta Lake adds support for relational semantics for both batch and streaming data operations, and enables the creation of a Lakehouse architecture in which Apache Spark can be used to process and query data in tables that are based on underlying files in the data lake.

## Task 1: Explore delta lake using a notebook ##
In this exercise, you'll use code in a notebook to explore delta lake in Azure Databricks.

In the Azure Databricks workspace portal for your workspace, in the sidebar on the left, select Workspace. Then select the ⌂ Home folder.

At the top of the page, in the ⋮ menu next to your user name, select Import. Then in the Import dialog box, select URL and import the notebook from https://github.com/MicrosoftLearning/dp-203-azure-data-engineer/raw/master/Allfiles/labs/25/Delta-Lake.ipynb

Connect the notebook to your cluster, and follow the instructions it contains; running the cells it contains to explore delta lake functionality.

    It is very important that you read all the instructions and explanations of what you are going to do.