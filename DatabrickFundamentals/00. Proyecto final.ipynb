{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingestión y Preparación de Datos\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"DataIngestion\").getOrCreate()\n",
    "\n",
    "# Cargar datos desde Azure Blob Storage\n",
    "data_path = \"<ruta_a_tu_almacenamiento_blob>\"\n",
    "raw_data = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Mostrar una vista previa de los datos\n",
    "raw_data.show()\n",
    "\n",
    "# Realizar transformaciones básicas\n",
    "data_cleaned = raw_data.dropna()\n",
    "data_cleaned = data_cleaned.dropDuplicates()\n",
    "\n",
    "# Guardar los datos limpiados en formato Parquet\n",
    "cleaned_data_path = \"<ruta_a_tu_almacenamiento_para_datos_limpiados>\"\n",
    "data_cleaned.write.parquet(cleaned_data_path)\n",
    "\n",
    "# Finalizar la sesión de Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 2: Transformaciones y Acciones en Apache Spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"DataTransformations\").getOrCreate()\n",
    "\n",
    "# Leer los datos limpiados\n",
    "data_cleaned_path = \"<ruta_a_tu_almacenamiento_para_datos_limpiados>\"\n",
    "data = spark.read.parquet(data_cleaned_path)\n",
    "\n",
    "# Aplicar transformaciones\n",
    "data_transformed = data.withColumn(\"new_column\", data[\"existing_column\"] * 2)\n",
    "\n",
    "# Realizar acciones y mostrar resultados\n",
    "data_transformed.show()\n",
    "\n",
    "# Guardar los datos transformados\n",
    "transformed_data_path = \"<ruta_a_tu_almacenamiento_para_datos_transformados>\"\n",
    "data_transformed.write.parquet(transformed_data_path)\n",
    "\n",
    "# Finalizar la sesión de Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 3: Entrenamiento y Evaluación de un Modelo de ML\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Inicializar la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"MLModelTraining\").getOrCreate()\n",
    "\n",
    "# Leer los datos transformados\n",
    "data_transformed_path = \"<ruta_a_tu_almacenamiento_para_datos_transformados>\"\n",
    "data = spark.read.parquet(data_transformed_path)\n",
    "\n",
    "# Preparar los datos para ML\n",
    "feature_columns = [\"feature1\", \"feature2\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data_ml = assembler.transform(data)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train_data, test_data = data_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "evaluation = model.evaluate(test_data)\n",
    "print(f\"RMSE: {evaluation.rootMeanSquaredError}\")\n",
    "\n",
    "# Finalizar la sesión de Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 4: Implementación de Delta Lake y Streaming Estructurado\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar la sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeStreaming\").getOrCreate()\n",
    "\n",
    "# Definir ruta de almacenamiento Delta\n",
    "delta_path = \"<ruta_a_tu_almacenamiento_delta_lake>\"\n",
    "\n",
    "# Lectura continua de datos en streaming\n",
    "data_stream = spark.readStream.format(\"csv\").option(\"header\", \"true\").load(\"<ruta_a_tu_fuente_streaming>\")\n",
    "\n",
    "# Escribir los datos en Delta Lake\n",
    "data_stream.writeStream.format(\"delta\").option(\"checkpointLocation\", \"<ruta_a_tu_checkpoint>\").start(delta_path)\n",
    "\n",
    "# Leer y mostrar datos de Delta Lake\n",
    "data_delta = spark.read.format(\"delta\").load(delta_path)\n",
    "data_delta.show()\n",
    "\n",
    "# Finalizar la sesión de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
